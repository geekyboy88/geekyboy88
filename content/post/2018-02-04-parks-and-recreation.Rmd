---
title: Parks and Recreation
author: geekyboy88
date: '2018-02-04'
slug: parks-and-recreation
categories:
  - Text
  - Text Analytics
  - R
tags:
  - analytics
  - TextAnalytics
  - R
---

<style type="text/css">

body, td {
   font-size: 18px;
}
code.r{
  font-size: 18px;
}
pre {
  font-size: 18px
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning=FALSE, echo=FALSE, comment=NA, message=FALSE)
```

## Parks and Recreation

Last year, I was introduced to the show Parks and Recreation, which is a [__Mockumentary__](https://en.wikipedia.org/wiki/Mockumentary) that revolved around the everyday office life of the Parks Department of Pawnee, Indiana. The main star of the show is Leslie Knope, who is a deputy director in that department. Being set in a government agency, we see how bureaucracy slows down her efforts to develop the town of Pawnee into a beautiful town.  

```{r loadpackages,echo=TRUE,results='hide'}
rm(list=ls())
packages = c("magrittr","rvest","plyr","dplyr","stringr","stringi","tidyr","lattice","tm","quanteda","SnowballC","Matrix","reshape2","tidytext","tidyverse","ggplot2","scales","knitr","igraph","ggraph")
lapply(packages, require, character.only = TRUE)
rm(packages)
```

### Web-scraping

To obtain the script and convert into text data, I used the springfield's links and the __rvest__ package in R to first pull out all the links to the scripts of the show, and subsequently ran a loop to scrape the episodes scripts before writing them into a __.txt__ file

```{r, eval=FALSE}
### Get links to all episodes
url = "https://www.springfieldspringfield.co.uk/episode_scripts.php?tv-show=parks-and-recreation"

links = url %>%
  read_html() %>%
  html_nodes("a") %>%
  html_attr("href") %>%
  as.data.frame()

names(links) = "url"

links %<>%
  filter(grepl("view_episode_scripts", url) == T) %>%
  mutate(season = substr(url, 64, 65),
         episode = substr(url, 67, 68))


### Get script for each episode and put into txt
for(i in 1:nrow(links)){
  url = paste0("https://www.springfieldspringfield.co.uk/",links$url[i] %>% as.character())
  script = url %>%
    read_html() %>%
    html_nodes(".scrolling-script-container") %>%
    html_text()
  script = gsub("\n", "", script)
  script = gsub("\t", "", script)
  script = gsub("\\\"", "", script)
  script %<>% trimws()
  
  fileCon = file(paste0("ParksAndRecreationS",links$season[i],"E",links$episode[i], ".txt"))
  sink(fileCon)
  cat(script)
  sink()
}
```

First, let's load the script as text data, remove all punctuation, whitespaces, and lastly to convert all the letters to lowercase for easy data wrangling.

```{r loaddata1, echo=T}
files = VCorpus(DirSource("../../static/data/text", recursive = T), readerControl = list(language = "en"))
```

```{r loaddata2}
files %<>% tm_map(stripWhitespace)
```

## Tidying the Data

For easy text data manipulation, I've used the packages described in [Text Mining in R](https://www.tidytextmining.com/) by Julia Silge and David Robinson. In the codes below, I converted the Corpus of text files that I have consolidated under "_files_"" into a dataframe "_parks_". This is a pretty neat solution, given that it allows me to have a look at the metadata, and also run data manipulations quite easily. Let's see where this takes us.

```{r tidying}
parks = tidy(files)
rm(files)
parks$episode = substr(parks$id, 19, 24)
```

## Most Common Words

The first thing that we could do is to identify what are the most common words that are found in the script. However, as you can tell from the tibble below, the most common words are those that are also most commonly used in the english language, such as __I__, __you__, __the__. However, this does not give us any indication of what this TV show, "__Parks and Recreation__" is about. 

```{r commonwords1}
tidy_parks = parks %>%
  select(id, episode, text) %>%
  unnest_tokens(word, text) %>%
  mutate(season = substr(episode, 2, 3))

tidy_parks %>%
  dplyr::count(word, sort = T) %>%
  head(10) %>%
  kable(col.names = c("Word","Word Count"))
```

##  Most Common Words

A common pre-processing step that many working on text analytics would do is to first remove the stop words. These are words that are commonly found in the english language (_what I mentioned previously_). 

After removing these words, we can immediately have some clue of what the show is about. We see words that are:

1. commonly used in speech (__yeah__, __gonna__ and __hey__), 
2. show names (__leslie__, __ron__, __ann__), and
3. related to the theme of the show (__god__, __love__, __people__, __pawnee__).

```{r commonwords2}
data("stop_words")

tidy_parks %>%
  anti_join(stop_words) %>%
  dplyr::count(word, sort = T) %>%
  head(20) %>%
  kable(col.names = c("Word","Word Count"))
```

## Name Mentions
 
The next thing on our minds would be to find out who are the main characters of this show. As mentioned previously, Leslie Knope is the main star of the show, and she is closely followed behind by her boss, Ron Swanson. The rest of the crew seem to have more or less the same level of show time with the rest.

```{r commonwords3}
characters = tidy_parks %>%
  dplyr::group_by(season) %>%
  dplyr::summarise(leslie = sum(ifelse(word %in% c("leslie","knope"), 1, 0)),
            ron = sum(ifelse(word %in% c("ron","swanson"),1,0)),
            ann = sum(ifelse(word %in% c("ann","perkins"),1,0)),
            tom = sum(ifelse(word %in% c("tom","haverford"),1,0)),
            andy = sum(ifelse(word %in% c("andy","dwyer"), 1, 0)),
            ben = sum(ifelse(word %in% c("ben","wyatt"), 1, 0)),
            april = sum(ifelse(word %in% c("april","ludgate"), 1, 0)),
            jerry = sum(ifelse(word %in% c("jerry","garry","larry","terry","girgich"), 1, 0)),
            chris = sum(ifelse(word %in% c("chris","traeger"), 1, 0)))

characters1 = characters %>% 
  melt()

characters1 %>%
  group_by(variable) %>%
  summarise(count = sum(value)) %>%
  arrange(desc(count))
                        
```

However, is there a difference between each character's appearances between the various seasons? Let's take a closer look.

From the chart below, we can derive a few insights:

1. Leslie Knope is definitely the main character of the show as she has the most number of name mentions through almost all seasons, save for the last one where she was overtaken slightly by her boss, Ron.
2. Ben Wyatt and Chris Traeger only appear towards the end of the second season. __Why did they only appear then though?__
3. Jerry had a slightly stand-out __season 5__.
4. April Ludgate was also more involved in the __final season__.


```{r commonwords4,fig.height=20,fig.width=13}
characters %<>%
  melt(id = "season") 

characters$variable %<>% as.character()

characters %<>%
  dplyr::group_by(season) %>%
  dplyr::mutate(pct = (value / sum(value)) %>% round(2),
         pos = sum(value) - cumsum(value) + value/2)

characters %>%
  ggplot(aes(x = season, y = value, group = variable, fill = variable)) +
  geom_bar(stat = "identity", position = position_stack()) +
  theme_light() +
  theme(text = element_text(size = 18),
        legend.position = "bottom") +
  labs(x = "Season", y = "Proportion of Name Mentions", fill = "Characters") +
  facet_grid(variable~., scales = "free") +
  geom_text(aes(label = paste0(100*pct, "%")), size = 7)
rm(characters)
```

## Understanding the Characters

The next thing that we would like to do is to understand a bit about the characters better. To do so, we will use a technique that tokenizes these words into __"n-grams"__. What I've done below is the simplest case of tokenizing by 2 words, and finding out how often a word appears with another word, and in doing so we hope to see if we can understand these characters better!

```{r characters1}
parks$season = substr(parks$episode, 2, 3)
parks_bigrams = parks %>%
    unnest_tokens(bigram, text, token = "ngrams", n = 2) %>%
  select(season, bigram)


parks_bigrams %<>%
  separate(bigram, c("word1","word2"), sep = " ") %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word) 

parks_bigrams %>%
  count(word1, word2, sort = T) %>%
  arrange(desc(n)) %>%
  head(20)
```

To start off, let's take a look at the words that tend to be associated with the main star of our show, __Leslie Knope__.

From the table below, we can make some inferences. 

1. She's a woman (_duh_) as she's addressed as a _ms_.
2. __Councilwoman__. This could suggest that somewhere in one of the seasons, Leslie Knope decided to run for __city council__.
3. __Griggs-Knope__. None other than her mother.
4. __Ron__. Her boss, who holds the position of __Director__ of the parks department.


```{r characters2}
leslie = parks_bigrams %>%
  filter(word1 %in% c("leslie","knope") | word2 %in% c("leslie","knope")) %>%
  count(word1, word2, sort = T) 

leslie = bind_rows(leslie[,c(1,3)],leslie[,c(2,3)])
leslie$word1 = ifelse(leslie$word1 %>% is.na() == T, leslie$word2, leslie$word1)
leslie$word2 = NULL

leslie %>%
  filter(!word1 %in% c("leslie","knope")) %>%
  group_by(word1) %>%
  summarise(count = sum(n)) %>%
  arrange(desc(count)) %>%
  head(15) %>%
  kable(col.names = c("Words Associated with Leslie Knope","Count of Word"))
rm(leslie)
```

We can do the same for __Ron Swanson__.

The words associated with Ron are slightly more interesting, in the sense that his name tends to be associated with the names of the other characters, which could suggest that he interacts with all his subordinates equally (_good boss or just him being the boss?_). Something else interesting also pops up, as we see Ron's ex-wife, __Tammy__, appear a decent number of times through the season.

```{r characters3}
ron = parks_bigrams %>%
  filter(word1 %in% c("ron","swanson") | word2 %in% c("ron","swanson")) %>%
  count(word1, word2, sort = T) 

ron = bind_rows(ron[,c(1,3)],ron[,c(2,3)])
ron$word1 = ifelse(ron$word1 %>% is.na() == T, ron$word2, ron$word1)
ron$word2 = NULL

ron %>%
  filter(!word1 %in% c("ron","swanson")) %>%
  group_by(word1) %>%
  summarise(count = sum(n)) %>%
  arrange(desc(count)) %>%
  head(15) %>%
  kable(col.names = c("Words Associated with Ron Swanson","Count of Word"))
rm(ron)
```

## Theme of each season

Let's turn our attention to something else that __n-grams__ can help us learn more about the content in the episodes.

What we will do next, is to find out what are the most common bi-grams in each of the 7 seasons of Parks and Recreation, and put them into a simple bar plot using the __tf-idf__, short for term frequency - inverse document frequency. What this allows us to do is to find out what are the top 10 most important words in each of the seasons. 

Let's take a look. In the first season, __Janine Retrespo__ seems to play a major role. She is a member of the __zoning board__ (_2nd most important word, see where this is going?_), and Leslie Knope was trying to get her approval to fill in the pit at __Sullivan Street__. 

```{r theme1,fig.height=18,fig.width=13}
parks_bigrams %>%
  unite(bigram, word1, word2, sep = " ") %>%
  count(season, bigram) %>%
  bind_tf_idf(bigram, season, n) %>%
  arrange(desc(season, tf_idf)) %>%
  group_by(season) %>%
  top_n(n = 10, wt = tf_idf) %>%
  ggplot(aes(x = reorder(bigram,tf_idf), y = tf_idf, fill = season)) +
  geom_bar(stat = "identity") +
  theme_classic() +
  theme(text = element_text(size = 18),
        legend.position = "none") +
  facet_grid(season~., scales = "free") +
  labs(x = "\nTerm Frequency - Inverse Docmument Frequency", y = "Top 10 Most Important Words by Season\n") +
  coord_flip()
```

Another way of representing could be to use a network graph, using the __ggraph__ package. Let's take a look at what we have.

As we can see below, many of the word relations are still within each of their own _mini_ networks, and it is only in the node __"hey"__ where we see a number of words that are inter-related. But this is of course a figure of speech, which is essentially what our data is, a script. So it would be difficult to find anything coherent in this case, as compared to if we were to use text data from a story book, where the sentences are well structured. 

```{r theme2,fig.height=12,fig.width=12}
parks_bigram_graph = parks_bigrams %>%
  count(word1, word2, sort = T) %>%
  filter(n > 15) %>%
  graph_from_data_frame()

set.seed(Sys.Date())

a <- grid::arrow(type = "closed", length = unit(.15, "inches"))

ggraph(parks_bigram_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()

```

I hope this gives you an insight to what the show __Parks and Recreation__ is about. I really enjoyed the show, and I hope that looking at the key words (and without me explaining most of the stuff), you might be curious to find out what words like __"Entertainment 720"__ and __"Johnny Karate"__ mean in the show. I'll stop here for now, but I'll be back with further analyses on parks and recreation. 

## Stay Tuned!!
