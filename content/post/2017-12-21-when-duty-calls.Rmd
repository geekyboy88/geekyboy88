---
title: When Duty Calls
author: geekyboy88
date: '2017-12-21'
slug: when-duty-calls
categories:
  - R
  - Text
  - Analytics
  - Mediacorp
tags:
  - R
  - TextAnalytics
  - Mediacorp
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning=FALSE, echo=FALSE, comment=NA, message=FALSE)
```

### My first official post

Alright. I realised that my first post wasn't supposed to be online, so I had to remove it for good reasons. Anyhow, I'm still keen on getting my first official data science post up, so here we go!

### Mediacorp Channel 8 Prime Time

To start off, I need to introduce some facts about myself. I not too recently got married, got my own house, and my own __television__. This gave me quite a bit of freedom as to what I wanted to watch after I come home from work each day (_particularly as parents always have first choice on what show is screening on the family tv_).

I must also admit that my mandarin isn't particularly strong despite coming from a bilingual education system, given that _english_ has always been the main form of communication in my life. Hence, to ensure that I do not lose sight of my mother tongue, I thought it would be good practice to spend some time watching TV shows in Chinese. Naturally, mediacorp's [channel 8](https://tv.toggle.sg/en/channel8/) 9pm prime time slot comes to mind (_as this is the time that I will definitely be home after work_) 

### When Duty Calls

So I got hooked at a time when they were showing [When Duty Calls](https://video.toggle.sg/en/video/series/when-duty-calls/ep1/520403), a show sponsored by MINDEF & MHA to commemorate 50 years of national service. Honestly, I thought that the show provided quite a bit of entertainment value, and the idea of national service very easily resonates with majority of us Singaporeans. 

### Additional entertainment value

However, beyond the show, I was also interested to find out what Singaporeans thought of this show. In this instance, one could think of no better source than HardwareZone's EDMW. Soon, I found out that there is an official dedicated thread for this show. So I thought, why not put this thread through some data work and see what I could get!

### Text Data

The data that I collected were the individual posts, and the timing of these postings. If you are interested, the official link can be found [here](http://forums.hardwarezone.com.sg/tv-mania-animemania-266/mediacorpse-50th-national-service-drama-5571280.html).

### When do people post?

The first thing I thought we would be interested to find out is: __when do people post__? From the chart below, we can see the huge cluster of posts that happen when the show starts. Well, this is not really surprising, as we find little reason for people to be on this hardwarezone thread at any other time of the day. We can also see that the number of comments start to pick up as we approach the end of the show, and slowly tapers off past 10pm-ish. Well, that's about bedtime for most people, so I guess its appropriate that the number of posts go down too.

```{r readdata}
library(magrittr)
library(dplyr)
library(stringr)
library(ggplot2)
library(stringr)
library(gtools)
library(wordcloud)
library(Stack)
library(wordVectors)
library(tm)

data = read.csv("../../static/data/theduty.csv", stringsAsFactors = F)
data$X = NULL
data$Hour = substr(data$Time1,1,2) %>% as.numeric()
data$Time1 = gsub("^.*, ","",data$Time)
data$Time1 = paste0(ifelse(substr(data$Time1,7,8) == "PM", 
                           data$Time1 %>% substr(1,2) %>% as.numeric() + 12, 
                           data$Time1 %>% substr(1,2) %>% as.numeric()), 
                    ":",  data$Time1 %>% substr(4,5))
data$Time1 = ifelse(data$Time1 %>% str_length() == 4, paste0("0",data$Time1), data$Time1)
data$Time1 = ifelse(data$Time1 %>% substr(1,2) == "24", gsub("24","00", data$Time1), data$Time1)
data %>% 
  ggplot(aes(x = Time1, fill = ..count..)) +
  geom_bar() +
  theme_classic() +
  labs(x = "Time / Minute", y = "Number of Posts", title = "Chart 1: Number of Posts by Time of the Day") +
  theme(text = element_text(size = 14),
        axis.text.x = element_text(size = 14, angle = 90, hjust = 1),
        axis.text.y = element_text(size = 14)) +
  scale_x_discrete(breaks = c("00:00","01:00","02:00","03:00","04:00","05:00","06:00","07:00","08:00","09:00","10:00","11:00","12:00","13:00","14:00","15:00","16:00","17:00","18:00","19:00","20:00","21:00","22:00","23:00")) +
  scale_fill_continuous(low="blue", high="red")
```

### Advertisement Breaks

For easier viewing, I also have a zoomed-in version of the number of posts by time starting from just before the show starts to just after it ends. The zoomed-in version provides an additional interesting analysis. __Can you spot when the advertisement breaks are?__

```{r advert}
data %>% 
  filter(Time1 %>% substr(1,2) %>% as.numeric() >= 20 & Time1 %>% substr(1,2) %>% as.numeric() <= 22) %>%
  ggplot(aes(x = Time1, fill = ..count..)) +
  geom_bar() +
  theme_classic() +
  labs(x = "Time / Minute", y = "Number of Posts", title = "Chart 2: Number of Posts by Time from 8pm to 11pm") +
  theme(text = element_text(size = 14),
        axis.text.x = element_text(size = 14, angle = 90, hjust = 1),
        axis.text.y = element_text(size = 14)) +
  scale_x_discrete(breaks = c("00:00","01:00","02:00","03:00","04:00","05:00","06:00","07:00","08:00","09:00","10:00","11:00","12:00","13:00","14:00","15:00","16:00","17:00","18:00","19:00","20:00","21:00","22:00","23:00")) +
  scale_fill_continuous(low="blue", high="red")
```

### Popular Actor / Actresses?

If you had taken a look at the wikipedia link that I had shared above, you would likely to have noticed that the cast of this TV show is relatively solid. But us as humans also have a tendency to pick our favorites, and I would suppose that in our conversations with others about the show, it is natural for us to talk more frequently about our favourite cast. Aggregated across an entire community, we can then find out which of the cast members is the fan favourite!

Make a guess? Who amongst the cast members do you think would be the most popular? (_Do not peek!_)

```{r, pops}
df = Corpus(VectorSource(data$Post))
df %<>% tm_map(removePunctuation, preserve_intra_word_dashes = T)
df %<>% tm_map(content_transformer(tolower))
df %<>% tm_map(removeNumbers)
df %<>% tm_map(stripWhitespace)
df1 = df %>% tm_map(removeWords, stopwords("en"))
characters = c("desmond","felicia","kym","paige","pierre","romeo","meixin","kim","cloudie","jasmine","poh","longbin","xianfeng","yixuan","zheng","sim","james","weiguo","zgp","fel","jun","guang","shane","richie","pow","koh","junguang","junguangs","jingyu","meiguang","jinbao","yongzheng","yiyun","ge","geping","ping")
df1 %<>% tm_map(removeWords, characters)

wcm = TermDocumentMatrix(df)
wcm1 = TermDocumentMatrix(df1)
rm(df, df1)

meta = wcm %>% as.matrix() %>% as.data.frame()
meta$total_count = rowSums(meta)

meta$rowname = meta %>% row.names()
c = meta %>% filter(rowname %in% characters)
meta$rowname = NULL
c = c[,c("total_count","rowname")] %>% as.data.frame
c$characters = ifelse(c$rowname %in% c("fel","jingyu"), "felicia",
                      ifelse(c$rowname %in% c("kim","poh","jinbao"), "pierre",
                             ifelse(c$rowname %in% c("jun","guang","junguang","junguangs"), "desmond",
                                    ifelse(c$rowname %in% c("longbin"),"romeo",
                                           ifelse(c$rowname %in% c("weiguo","pow"), "shane",
                                                  ifelse(c$rowname %in% c("koh","xianfeng"),"richie",
                                                         ifelse(c$rowname %in% c("zheng","yongzheng","ge","geping","ping"), "zgp",
                                                                ifelse(c$rowname %in% c("yixuan"), "paige",
                                                                       ifelse(c$rowname %in% c("seah"), "james",
                                                                              ifelse(c$rowname %in% c("sim","yiyun","cloudie"), "jasmine",
                                                                                     ifelse(c$rowname %in% c("meiguang"), "kym",
                                                                                            c$rowname)))))))))))


# who is the most popular (talked about) mediacorp actor/actress?
c %>% 
  group_by(characters) %>%
  summarize(count = sum(total_count)) %>%
  ggplot(aes(x = reorder(characters,-count), y = count, fill = count)) +
  geom_bar(stat = "identity") +
  theme_classic() +
  theme(axis.text.x = element_text(size = 14, hjust = 1, angle = 90, colour = "black"), legend.position = "none", text = element_text(size = 14)) +
  labs(x = "Mediacorp Actor/Actresses", y = "Number of word mentions", title = "Chart 3: Number of Name Mentions by Cast-Member") +
  scale_fill_continuous(low = "blue", high = "red")
```

Well, I'm sure there's nothing stopping you from scrolling down to take a look. Anyhow, if you follow the local media scene, I'm sure you would be rougly familiar with [Paige Chua](https://en.wikipedia.org/wiki/Paige_Chua), who has come in top as our fan favourite cast member for "When Duty Calls". Following close behind is [Desmond Tan](https://en.wikipedia.org/wiki/Desmond_Tan/), who (as I just read from Wikipedia) has a fan club called __Destanation__ and is considered one of the 8 __Dukes__ of Mediacorp. [Pierre Png](https://en.wikipedia.org/wiki/Pierre_Png), well you all should know who he is right. He came in third.

### WordCloud

Lastly, I thought it would also be interesting to find out which are the most commonly words that are used in the thread. What I did was to parse each post (_usually in sentences or at least phrases_) into words, following which, for each word, I counted the number of occurances of each word, and summed up across the entire thread. As I am typing this, I realise I should have explained this earlier when finding out who the most popular cast member was, but I'm a little too lazy to backtrack, so... nevermind!

In the word cloud, the size of the word reflects the number of occurances of that particular word, i.e. the __larger__ the word, the more frequently it was used. I wouldn't be spending time to talk much about the wordcloud. I think that it is rather self-explanatory, and I hope you have a good laugh when you encounter some "familiar" words :).

```{r wordcloud, fig.width=12, fig.height=8}
meta1 = wcm1 %>% as.matrix() %>% as.data.frame()
meta1$total_count = rowSums(meta1)
pal2 <- brewer.pal(8,"Dark2")
wordcloud(row.names(meta1), meta1$total_count, min.freq = 3, scale = c(8,.2),max.words = 100, random.order = F, random.color = FALSE, colors= pal2, fixed.asp = F, rot.per = 0)
```

Hope you enjoy my first official post!




